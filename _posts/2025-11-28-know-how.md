---
tags: [statistics, data]
---

Ranja Sarkar

-----

Let me start with an acronym iid; **iid** is **independent & identically distributed**, which implies a collection of random variables where each variable has a probability distribution same as others in the collection and all of them are mutually independent. 

The **law of large numbers** states that when this collection is a large sample, the sample mean converges to the true mean if it exists. The law applies to the average obtained from a large number of repeated trials and claims that this average converges to the expected (or mean) value. 

The **central limit theorem (CLT)** states that, given a sufficiently large sample size, the sampling distribution of the mean for a random variable approximates a normal distribution regardless of itâ€™s distribution in the population. 

CLT is vital for two reasons, the normality assumption and the precision of estimates. The normality assumption is vital for parametric hypothesis tests of the mean. One might think that these tests are not valid when the data are non-normally distributed. However, if our sample size is large enough, CLT kicks in and produces sampling distributions that approximate a normal distribution. This fact allows us to use these hypothesis tests even when our data are non-normally distributed as long as the sample size is large enough. The 'precision of estimates' property of CLT becomes relevant when we use a sample to estimate the mean of an entire population. With a larger sample size, the sample mean is more likely to be close to the real population mean. In other words, our estimate is precise. **Accuracy** and **precision** cannot be used interchangeably. Accuracy is being true to intention (degree of closeness of measured value to true value) while precision is true to itself (degree of closeness of repeated measured values).

Descriptive statistics only helps draw inference about a sample. Inferential statistics helps draw inference about the population. If the sample is representive of the population, both stats yield same results.

**Probability** and **likelihood** are different terms. Probability is finding the chance of outcomes given a data distribution, and likelihood is finding the most likely distribution given the outcomes. Since likelihood is not probability, one can obtain likelihood greater than 1. 
Having stated this, it may be helpful to conceptually think of likelihood as "probability of the data given the parameters".



