---
tags: [statistics, data]
---

Ranja Sarkar

-----

Let me start with an acronym iid, widely used in the world of statistics.
**iid** is **independent & identically distributed**, which implies a collection of random variables where each variable has a probability distribution same as others in the collection and all of them are mutually independent. The **law of large numbers** states that when this collection is a large sample, the sample mean converges to the true mean if it exists. The law applies to the average obtained from a large number of repeated trials and claims that this average converges to the expected (or mean) value. 

The **central limit theorem (CLT)** states that, given a sufficiently large sample size, the sampling distribution of the mean for a random variable approximates a normal distribution regardless of itâ€™s distribution in the population. 

CLT is vital for two reasons, the normality assumption and the precision of estimates. The normality assumption is vital for parametric hypothesis tests of the mean. One might think that these tests are not valid when the data are non-normally distributed. However, if our sample size is large enough, CLT kicks in and produces sampling distributions that approximate a normal distribution. This fact allows us to use these hypothesis tests even when our data are non-normally distributed as long as the sample size is large enough. The 'precision of estimates' property of CLT becomes relevant when we use a sample to estimate the mean of an entire population. With a larger sample size, the sample mean is more likely to be close to the real population mean. In other words, the estimate becomes precise. **Accuracy** and **precision** cannot be used interchangeably. Accuracy is being true to intention (degree of closeness of measured value to true value) while precision is true to itself (degree of closeness of repeated measured values).

Descriptive statistics only helps draw inference about a sample. Inferential statistics helps draw inference about the population. If the sample is representative of the population, both stats yield same results.

**Probability** and **likelihood** are different terms. Probability is finding the chance of outcomes given a data distribution, and likelihood is finding the most likely distribution given the outcomes. Since likelihood is not probability, one can obtain likelihood greater than 1. 
Having stated this, it may be helpful to conceptually think of likelihood as "probability of the data given the parameters".

-----

Depending on our objective, we perform a statistical test. 

For example, if the objective is to find the correlation between two variables or groups, we check the Pearson correlation coefficient only if the data is normally distributed. Due to this assumption, it's a parametric test. If there's no assumption, the test becomes non-parametric and usually a Spearman correlation coefficient is checked. So the selection of a test depends on the objectove and the type of data. 

Now, association between two variables is measured by correlation. Interation between two variables is different; if two variables interact, they may or may not be [associated](https://www.theanalysisfactor.com/interaction-association/).

Another example is if the goal is to predict a target from one or more variables, we perform regression which is a parametric test. A T-test (parametric) measures whether a given cofficient/weight is significantly different than zero. 

If we have to compare unpaired or independent groups, we perform unpaired T-test, or a non-parametric test like Mann-Whitney test depending on the data. [Analysis of variance (ANOVA) and linear regression](https://www.linkedin.com/pulse/why-anova-linear-regression-same-analysis-the-analysis-factor-llc-tn8ff) are the same, just the results of the analyses are presented in different ways. 



