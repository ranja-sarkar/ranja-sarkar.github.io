---
tags: [statistics, data]
---

Ranja Sarkar

-----

Let me start with an acronym iid, widely used in the world of statistics.
**iid** is **independent & identically distributed**, which implies a collection of random variables where each variable has a probability distribution same as others in the collection and all of them are mutually independent. The **law of large numbers** states that when this collection is a large sample, the sample mean converges to the true mean if it exists. The law applies to the average obtained from a large number of repeated trials and claims that this average converges to the expected (or mean) value. 

The **central limit theorem (CLT)** states that, given a sufficiently large sample size, the sampling distribution of the mean for a random variable approximates a normal distribution regardless of it’s distribution in the population. Below is the Wikipedia defnition.

<img width="446" height="82" alt="stats" src="https://github.com/user-attachments/assets/b39b31d9-170b-4487-8050-fc806e7fb43d" />


CLT is vital for two reasons, the normality assumption and the precision of estimates. The normality assumption is vital for parametric hypothesis tests of the mean. One might think that these tests are not valid when the data are non-normally distributed. However, if our sample size is large enough, CLT kicks in and produces sampling distributions that approximate a normal distribution. This fact allows us to use these hypothesis tests even when our data are non-normally distributed as long as the sample size is large enough. The 'precision of estimates' property of CLT becomes relevant when we use a sample to estimate the mean of an entire population. With a larger sample size, the sample mean is more likely to be close to the real population mean. In other words, the estimate becomes precise. **Accuracy** and **precision** cannot be used interchangeably. Accuracy is being true to intention (degree of closeness of measured value to true value) while precision is true to itself (degree of closeness of repeated measured values).

Descriptive statistics only helps draw inference about a sample. Inferential statistics helps draw inference about the population. If the sample is representative of the population, both stats yield same results.

**Probability** and **likelihood** are different terms. Probability is finding the chance of outcomes given a data distribution, and likelihood is finding the most likely distribution given the outcomes. Since likelihood is not probability, one can obtain likelihood greater than 1. 
Having stated this, it may be helpful to conceptually think of likelihood as "probability of the data given the parameters".

# Tests & measures

Depending on our objective, we perform a **statistical test**. By objective we mean, what do we want to measure? 

<img width="1261" height="314" alt="tt" src="https://github.com/user-attachments/assets/49a52318-91c5-4405-853f-4d14af1a17b9" />


For example, if the objective is to find the correlation between two variables or groups, we check the Pearson correlation coefficient only if the data is normally distributed. Due to this assumption, it's a parametric test. If there's no assumption, the test becomes non-parametric and usually a Spearman correlation coefficient is checked. So the selection of a test depends on the objective and the type of data. 

Now, association between two variables is measured by correlation. Interation between two variables is different; if two variables interact, they may or may not be [associated](https://www.theanalysisfactor.com/interaction-association/).

Another example is if the goal is to predict a target from one or more variables, we perform regression which is a parametric test. A T-test (parametric) measures whether a given cofficient/weight is significantly different than zero. 

If we have to compare unpaired or independent groups, we perform unpaired T-test, or a non-parametric test like Mann-Whitney test depending on the data. [Analysis of variance (ANOVA) and linear regression](https://www.linkedin.com/pulse/why-anova-linear-regression-same-analysis-the-analysis-factor-llc-tn8ff) are the same, just the results of the analyses are presented in different ways. 

<img width="1513" height="361" alt="tests" src="https://github.com/user-attachments/assets/d7979854-3976-4fd3-9343-f98947d7e1eb" />

-----

**Data visualization** is a very essential element of data exploration. For example, a [scatter plot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) helps us understand which measure (correlation coefficient etc.) to use for the data. The plot helps identify linear, nonlinear relationships between variables and spot outliers (if any) which may influence the correlation. 

While a box plot visually represents inter-quartile range (twice of quartile deviation), a [violin plot](https://plotly.com/python/violin/) shows the shape or density distribution of data. A violin plot must be used to explore skewed data. 

<img width="357" height="284" alt="vp" src="https://github.com/user-attachments/assets/314d9b9b-0fd9-4c9c-9428-3cd4405f8516" />

-----

There are two schools of thought or approaches in statistical testing. One is frequentist, another is Bayesian. The former framework is how often an outcome happens over repeated runs of the experiment/test. It’s an objective view of whether an experiment is repeatable. The latter is a subjective view of the same. [Bayesian](https://github.com/ranja-sarkar/ranja-sarkar.github.io/blob/3dab2be104f4905452c182d3e1d53218b277e8ad/_posts/assets/Springer%20Nature_2020.pdf) takes into account how much faith we have in our results. It includes prior knowledge about the data and personal beliefs about the results (likelihood). We start with a belief (prior) and we strengthen/weaken the prior with each evidence/datapoint that is, we update the belief to a degree. The updated belief is posterior probability.

Tests also measure **central tendency** and **dispersion** of data. For example, **mode** is the best measure of central tendency for nominal qualitative (categorical) data and **median** is the best for ordinal qualitative data. For interval/ratio types of quantitative (numeric) data, median is the best for a skewed distribution amd mean for a not skewed distribution.

<img width="489" height="214" alt="sk" src="https://github.com/user-attachments/assets/1bd51666-8008-4c78-ac02-22fd023df229" />

The first 4 [moments](https://gregorygundersen.com/blog/2020/04/11/moments/) of a distribution are mean, variance, skewness, kurtosis. While a symmetric distribution always has zero skewness of zero, the opposite claim is not always true that is, a distribution with zero skewness may be asymmetric.

<img width="283" height="248" alt="ks" src="https://github.com/user-attachments/assets/83041fd3-9568-4dd9-a712-4245bb293b26" />


While skewness is a measure of the relative size of the two tails of a distribution, it is positive or negative depending on which tail is larger, kurtosis is a measure of the size of two tails together relative to the distribution.

There are **power transformations** that variables in a dataset need to undergo if they follow right-skewed or left-skewed distributions. Power transforms refer to a class of techniques utilizing a power function (logarithm or exponent) to make the probability distribution of the variable Gaussian (normal) or Gaussian-like. 

The [Box-Cox transformation](https://feature-engine.trainindata.com/en/latest/user_guide/transformation/index.html) is a generalization of the power transformations family. They find a parameter 'lambda' that best transforms the variable, for example lambda = -1 is a reciprocal transform, lambda = 0 is a log transform, lambda = 0.5 is a square root transform.

<img width="225" height="41" alt="bc" src="https://github.com/user-attachments/assets/e5f64f82-e372-4410-a45d-2e038912351c" />

While a simple measure of data dispersion is its range, **variance** and standard deviation best serve the purpose.

<img width="457" height="162" alt="sd" src="https://github.com/user-attachments/assets/cc9e5a65-5ff3-4cde-916f-fe01d16ac3ad" />

The standard deviation (s) of a data sample is the degree to which individuals (records/observations) within the sample differ from the mean of the sample. The **standard error** of the sample (size n) is an estimate of how far its mean is likely to be away from the population mean. It is defined as s/sqrt(n).

**Covariance** is another measure. It measures how two variables in a dataset move wrt each other. 

<img width="502" height="182" alt="cov" src="https://github.com/user-attachments/assets/b53b3e0c-f3b1-4ff5-8a49-f865cfdcac45" />

Yes! covariance is related to correlation. The former gives a 2D picture (x and y) and the latter is a 1D picture (x or y).

<img width="293" height="78" alt="corr" src="https://github.com/user-attachments/assets/b4f054c1-e0b3-4dd3-bd4e-338f71bbb482" />

So we are back to correlation, the measure and topic with which we started our discussion here.
We shall see how correlation and covariance are diferent.


<img width="443" height="112" alt="co0" src="https://github.com/user-attachments/assets/1d652478-aa2f-46c0-b73e-528173132df4" />



-----

